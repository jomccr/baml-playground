# ----------------------------------------------------------------------------
#
#  Welcome to Baml! To use this generated code, please run the following:
#
#  $ pip install baml
#
# ----------------------------------------------------------------------------

# This file was generated by BAML: please do not edit it. Instead, edit the
# BAML files and re-generate this code using: baml-cli generate
# baml-cli is available with the baml package.

_file_map = {

    "article.baml": "\nclass Fact {\n  value int @description(#\"\n    If the value is in currency, round to the nearest whole dollar. For example, always expand \"$12.8 millions\" to 12800000 \"USD\"\n  \"#)\n  units string\n  start_date string\n  end_date string?\n  location string? @description(#\"\n    A plain text description of where this fact can be found on the PDF\n  \"#)\n}\n\nclass FilingInformation {\n  company string\n  description string\n  derivation Fact? @description(#\"\n    If asked for multi-fact calculation, put the *final calculation* here\n  \"#)\n  facts Fact[] @description(#\"\n    If asked for a multi-fact calculation, put the *base facts* here\n  \"#)\n}\n\nclass CalculatorAPI {\n  intent \"basic_calculator\"\n  operation \"add\" | \"subtract\" | \"multiply\" | \"divide\"\n  numbers float[]\n}\n\nfunction RequestCalculation(message: string) -> CalculatorAPI {\n  client Gemini\n  prompt #\"\n    Given a message, extract info.\n    {{ ctx.output_format }}\n    {{ _.role(\"user\") }} {{ message }}\n  \"#\n}\n\nfunction SearchDoc(document: pdf, info: string) -> FilingInformation {\n  client Gemini\n  prompt #\"\n    Extract the information from the document below.\n\n    Info: {{info}}\n    Document: {{document}}\n\n    Respond in well-formatted json\n    {{ ctx.output_format }}\n  \"#\n}\n",
    "clients.baml": "client<llm> CustomGpt5Nano {\n  provider openai\n  options {\n    model \"gpt-5-nano\"\n    api_key env.OPENAI_API_KEY\n  }\n}\n\nclient<llm> Gemini {\n  provider google-ai\n  options {\n    model \"gemini-2.5-flash\"\n    api_key env.GEMINI_API_KEY\n  }\n}\n\nclient<llm> Lmstudio {\n  provider openai-generic\n  options {\n    model \"google/gemma-3-4b\"\n    base_url \"http://localhost:1234/v1\"\n  }\n}\n",
    "generators.baml": "// This helps use auto generate libraries you can use in the language of\n// your choice. You can have multiple generators if you use multiple languages.\n// Just ensure that the output_dir is different for each generator.\ngenerator target {\n    // Valid values: \"python/pydantic\", \"typescript\", \"ruby/sorbet\", \"rest/openapi\"\n    output_type \"python/pydantic\"\n\n    // Where the generated code will be saved (relative to baml_src/)\n    output_dir \"../\"\n\n    // The version of the BAML package you have installed (e.g. same version as your baml-py or @boundaryml/baml).\n    // The BAML VSCode extension version should also match this version.\n    version \"0.206.1\"\n\n    // Valid values: \"sync\", \"async\"\n    // This controls what `b.FunctionName()` will be (sync or async).\n    default_client_mode sync\n}\n",
}

def get_baml_files():
    return _file_map